[
["index.html", "Creating APIs in R with Plumber Chapter 1 Introduction 1.1 Web APIs 1.2 Installation", " Creating APIs in R with Plumber Jeff Allen Chapter 1 Introduction The R Programming Language (R Core Team 2013) has become one of the most dominant programming languages for data analysis and visualization in recent years. At the same time, web services have become a common language for allowing various systems to interact with one another. The plumber R package (Trestle Technology, LLC 2017) allows users to expose existing R code as a service available to others on the Web. Plumber is best illustrated with an example: # plumber.R #&#39; Echo the parameter that was sent in #&#39; @param msg The message to echo back. #&#39; @get /echo function(msg=&quot;&quot;){ list(msg = paste0(&quot;The message is: &#39;&quot;, msg, &quot;&#39;&quot;)) } #&#39; Plot out data from the iris dataset #&#39; @param spec If provided, filter the data to only this species (e.g. &#39;setosa&#39;) #&#39; @get /plot #&#39; @png function(spec){ myData &lt;- iris title &lt;- &quot;All Species&quot; # Filter if the species was specified if (!missing(spec)){ title &lt;- paste0(&quot;Only the &#39;&quot;, spec, &quot;&#39; Species&quot;) myData &lt;- subset(iris, Species == spec) } plot(myData$Sepal.Length, myData$Petal.Length, main=title, xlab=&quot;Sepal Length&quot;, ylab=&quot;Petal Length&quot;) } Even without knowing R, you can probably get a rough idea for what the above Plumber API will do. The first function above defines the /echo endpoint which simply echoes back the text that it was sent. The second function generates a plot based on Edgar Anderson’s famous Iris Dataset; it includes a filter that allows the caller to subset the dataset to a particular species. Plumber makes use of these comment “annotations” above your functions to define the web service. When you feed the above file into Plumber, you’ll get a runnable web service that other systems can interact with over a network. 1.1 Web APIs The Hypertext Transfer Protocol (HTTP) is the dominant medium by which information is exchanged on the Internet. An Application Programming Interface (API) is a broad term that defines the rules that guide your interaction with some software. In the case of HTTP APIs, you have a defined set of endpoints that accept particular inputs. Plumber translates the annotations you place on your functions into an HTTP API that can be called from other machines on your network. If you execute your Plumber API on a public server, you can even make your API available to the public Internet. HTTP APIs have become the predominant language by which software communicates. By creating an HTTP API, you’ll empower your R code to be leveraged by other services – whether they’re housed inside your organization or hosted on the other side of the world. Here are just a few ideas of the doors that are opened to you when you wrap your R code in a Plumber API: Software written in other languages in your organization can run your R code. Your company’s Java application could now pull in a custom ggplot2 graph that you generate on-demand, or a Python client could query a predictive model defined in R. You can have some third-party receive emails on your behalf and then notify your Plumber service when new messages arrive. You could register a “Slash Command” on Slack, enabling you to execute your R function in response to a command being entered in Slack. You can write JavaScript code that queries your Plumber API from a visitor’s web browser. Even further, you could use Plumber exclusively as the back-end of an interactive web application. 1.2 Installation Plumber is hosted on CRAN, so you can download and install the latest stable version and all of its dependencies by running: install.packages(&quot;plumber&quot;) Alternatively, if you’d like to run the latest unstable development version of plumber, you can install it from its GitHub repository using the devtools package. install.packages(&quot;devtools&quot;) devtools::install_github(&quot;trestletech/plumber&quot;) Once you have plumber installed, see the Quickstart for information on how to get up-and-running with Plumber in minutes. "],
["quickstart.html", "Chapter 2 Quickstart 2.1 Specifying the Inputs 2.2 Customizing The Output", " Chapter 2 Quickstart Plumber allows you to create APIs by merely decorating your existing R code with special annotations. The example below shows a file named plumber.R (the conventional name for Plumber APIs) which defines an API. # plumber.R #&#39; Echo the parameter that was sent in #&#39; @param msg The message to echo back. #&#39; @get /echo function(msg=&quot;&quot;){ list(msg = paste0(&quot;The message is: &#39;&quot;, msg, &quot;&#39;&quot;)) } #&#39; Plot out data from the iris dataset #&#39; @param spec If provided, filter the data to only this species (e.g. &#39;setosa&#39;) #&#39; @get /plot #&#39; @png function(spec){ myData &lt;- iris title &lt;- &quot;All Species&quot; # Filter if the species was specified if (!missing(spec)){ title &lt;- paste0(&quot;Only the &#39;&quot;, spec, &quot;&#39; Species&quot;) myData &lt;- subset(iris, Species == spec) } plot(myData$Sepal.Length, myData$Petal.Length, main=title, xlab=&quot;Sepal Length&quot;, ylab=&quot;Petal Length&quot;) } This file defines two Plumber “endpoints.” One is hosted at the path /echo and simply echoes the message passed in; the other is hosted at the path /plot and returns an image showing a simple R plot. If you haven’t installed plumber yet, see the installation section. Once you have plumber installed, you can use the plumber::plumb function to translate this R file into a Plumber API: pr &lt;- plumber::plumb(&quot;plumber.R&quot;) The pr object now encapsulates all the logic represented in your plumber.R file. The next step is to bring the API to life using the run method: pr$run() You should see a message about your API running on your computer on port 8000. The API will continue running in your R session until you press the Esc key. If you’re running this code locally on your personal machine, you should be able to open http://localhost:8000/echo or http://localhost:8000/plot in a web browser to test your new API endpoints. If you’re using a tool like RStudio Server to run your R code on a remote machine, you should see the networking section for help with visiting your API. The /echo endpoint should show output resembling the following. { &quot;msg&quot;: [&quot;The message is: &#39;&#39;&quot;] } The /plot endpoint will show you a simple plot of some data from the iris dataset. If you see something like the above: congratulations! You’ve just created your first Plumber API! You’ve already exercised your API from a web browser, but there’s nothing stopping you from leveraging this API from third-party tools or a client developed in R or any other programming language. 2.1 Specifying the Inputs You may have noticed that the functions that define our endpoints accept parameters. These parameters allow us to customize the behavior of our endpoints. One of the ways to do this is using “query strings” which are a way of passing variables into an HTTP API. If you visit http://localhost:8000/plot?spec=setosa, you should see a similar graph to the one you saw before, but now the dataset has been filtered to only include the “setosa” species in the iris dataset. As you might have guessed, the spec=setosa portion of the URL sets the spec parameter to setosa. More details on how Plumber processes inputs are available in the Routing &amp; Input Chapter. 2.2 Customizing The Output In the previous example, you saw one endpoint that rendered into JSON and one that produced an image. Unless instructed otherwise, Plumber will attempt to render whatever your endpoint function returns as JSON. However, you can specify alternative “serializers” which instruct Plumber to render the output as some other format such as HTML (@html), PNG (@png), or JPEG (@jpeg). #&#39; @get /hello #&#39; @html function(){ &quot;&lt;html&gt;&lt;h1&gt;hello world&lt;/h1&gt;&lt;/html&gt;&quot; } This endpoint would produce something like the following, when visited. It also sets the appropriate Content-Type header so that a browser that visits this page would know to render the result as HTML. &lt;html&gt;&lt;h1&gt;hello world&lt;/h1&gt;&lt;/html&gt; You can even provide your own custom serializers and define how to translate the R object produced by your endpoint into the bits that will be produce Plumber’s HTTP response. You can find more details in the Rendering &amp; Output chapter. "],
["routing-and-input.html", "Chapter 3 Routing &amp; Input 3.1 Endpoints 3.2 Filters 3.3 Dynamic Routes 3.4 Typed Dynamic Routes 3.5 Static File Handler 3.6 Input Handling", " Chapter 3 Routing &amp; Input Plumber’s first job is to execute R code in response to incoming HTTP requests, so it’s important to understand how incoming HTTP requests get translated into the execution of R functions. An incoming HTTP request must be “routed” to one or more R functions. Plumber has two distinct families of functions that it handles: endpoints and filters. Typically, when a request arrives to a Plumber router, Plumber begins by passing that request through its filters. Once the request has been processed by all the filters, the router will look for an endpoint that can satisfy the incoming request. If it finds one, it will invoke the endpoint and respond to the incoming request using the value the endpoint returned. If no endpoint matches the request, then a 404 Not Found error will be returned (the behavior of which can be controlled the set404Handler method). 3.1 Endpoints Endpoints are the terminal step in the process of serving a request. An endpoint can simply be viewed as the logic that is ultimately responsible for generating a response to a particular request. A request will be checked against each available endpoint until it finds an endpoint willing to serve it at which point it stops looking; i.e. a request will not ever be processed by more than one endpoint. You create an endpoint by annotating a function like so: #&#39; Echo the parameter that was sent in #&#39; @get /echo function(msg=&quot;Hi!&quot;){ list(msg = paste(&quot;The message is: &quot;, msg)) } This annotation specifies that this function is responsible for generating the response to any GET request to /hello. The value returned from the function will be used as the response to the request (after being run through a serializer to e.g. convert the response into JSON). In this case, a GET response to /hello would return the content [&quot;hello world&quot;] with a JSON Content-Type. The annotations that generate an endpoint include: @get @post @put @delete @head These map to the HTTP methods that an API client might send along with a request. By default when you open a page in a web browser, that sends a GET request to the API. But you can use other API clients (or even JavaScript inside of a web browser) to form HTTP requests using the other methods listed here. There are conventions around when each of these methods should be used which you can read more about here. Note that some of these conventions carry with them security implications, so it’s a good idea to follow the recommended uses for each method until you fully understand why you might deviate from them. Note that a single endpoint can support multiple verbs. The following function would be used to service any incoming GET, POST, or PUT request to /cars. #&#39; @get /cars #&#39; @post /cars #&#39; @put /cars function(){ ... } 3.2 Filters Plumber filters can be used to define a “pipeline” for handling incoming requests. This allows API authors to break down complex logic into a sequence of independent, understandable steps. Unlike endpoints, a request may go through multiple Plumber filters before a response is generated. Typically, a Plumber router will pass a request through all the defined filters before it attempts to find an endpoint to satisfy the request. However, endpoints can “preempt” particular filters if they want to be considered for execution before some filter(s) registered in the router. // TODO: graphic Filters can do one of three things in handling a request: Forward control onto the next handler, potentially after mutating the request. Return a response itself and not forward to subsequent handlers Throw an error These three options, and why each might be desired, are discussed below. 3.2.1 Forward to Another Handler The most common behavior for a filter is to pass on the request to the next handler after mutating the incoming request or invoking some external side-effect. One common use case is to use a filter as a request logger: #* Log some information about the incoming request #* @filter logger function(req){ cat(as.character(Sys.time()), &quot;-&quot;, req$REQUEST_METHOD, req$PATH_INFO, &quot;-&quot;, req$HTTP_USER_AGENT, &quot;@&quot;, req$REMOTE_ADDR, &quot;\\n&quot;) plumber::forward() } This filter is straightfoward: it invokes an external action (logging) and then calls forward() to pass control to the next handler in the pipeline (another filter or an endpoint). Because the req and res parameters in Plumber are based on R environments, they exhibit “pass-by-reference” behavior. This means that changes that are made in one filter on the req or res object will be visible to other filters or endpoints also touching this same request or response. A similar filter may mutate some state on the request or response object it’s given. #* @filter setuser function(req){ un &lt;- req$cookies$user # Make req$username available to endpoints req$username &lt;- un plumber::forward() } In this case, the req object is going to be extended to have an additional property named username which represents a value looked up from a cookie. This req$username property would be available to all subsequent filters and endpoints processing this request. (Note that this example is not a secure system for authentication; see the section on using cookies to store state for a longer discussion on why.) Once it has modified the request object, it passes control to the next handler using forward(). 3.2.2 Return a Response It is also possible for filters to return a response. You may want to check that the request satisfies some constraint (like authentication) and – in certain cases – return a response without invoking any additional handlers. For example, a filter could be used to check that a user has authenticated. #* @filter checkAuth function(req, res){ if (is.null(req$username)){ res$status &lt;- 401 # Unauthorized return(list(error=&quot;Authentication required&quot;)) } else { plumber::forward() } } A common cause of errors in Plumber APIs is forgetting to invoke forward() in your filters. In such a filter, the result of the last line will be silently returned as the response to the incoming request. This can cause your API to exhibit very odd behavior depending on what’s being returned. When you’re using filters, be sure to carefully audit all code paths to ensure that you’re either calling forward(), causing an error, or intentionally returning a value. 3.2.3 Throw an Error Finally, a filter can throw an error. This can occur if a mistake is made in the code defining the filter or if the filter intentionally invokes stop() to trigger an error. In this case, the request will not be processed by any subsequent handlers and will immediately be sent to the router’s error handler. See router customization for more details on how to customize this error handler. 3.3 Dynamic Routes In addition to having hard-coded routes like /hello, Plumber endpoints can have dynamic routes. Dynamic routes allow endpoints to define a more flexible set of paths against which they should match. A common REST convention is to include the identifier of an object in the API paths associated with it. So to lookup information about user #13, you might make a GET request to the path /users/13. Rather than having to register routes for every user your API might possible encounter, you can use a dynamic route to associate an endpoint with a variety of paths. users &lt;- data.frame( uid=c(12,13), username=c(&quot;kim&quot;, &quot;john&quot;) ) #&#39; Lookup a user #&#39; @get /users/&lt;id&gt; function(id){ subset(users, uid==id) } This API uses the dynamic path /users/&lt;id&gt; to match any request that is of the form /users/ followed by some path element like a number or letters. In this case, it will return information about the user if a user with the associated ID was found, or an empty object if not. You can name these dynamic path elements however you’d like, but note that the name used in the dynamic path must match the name of the parameter for the function (in this case, both id). You can even do more complex dynamic routes like: #&#39; @get /user/&lt;from&gt;/connect/&lt;to&gt; function(from, to){ # Do something with the `from` and `to` variables... } In both the hard-coded and dynamic examples given above, the parameters will be provided to the function as a character string. 3.4 Typed Dynamic Routes Unless otherwise instructed, all parameters passed into plumber endpoints from query strings or dynamic paths will be character strings. For example, consider the following API. #&#39; @get /type/&lt;id&gt; function(id){ list( id = id, type = typeof(id) ) } Visiting http://localhost:8000/types/14 will return: { &quot;id&quot;: [&quot;14&quot;], &quot;type&quot;: [&quot;character&quot;] } If you only intend to support a particular data type for a particular parameter in your dynamic route, you can specify the desired type in the route itself. #* @get /user/&lt;id:int&gt; function(id){ next &lt;- id + 1 # ... } #* @post /user/activated/&lt;active:bool&gt; function(active){ if (!active){ # ... } } Specifying the type of a dynamic path element will also narrow the paths that will match the endpoint. For instance, the path /users/123 will match the first endpoint, but /users/8e3k will not, since 8e3k is not an integer. The following details the mapping of the type names that you can use in your dynamic types and how they map to R data types. R Type Plumber Name logical bool, logical numeric double, numeric integer int 3.5 Static File Handler Plumber includes a static file server which can be used to host directories of static assets such as JavaScript, CSS, or HTML files. These servers are fairly simple to configure and integrate into your plumber application. #* @assets ./files/static list() This example would expose the local directory ./files/static at the default /public path on your server. So if you had a file ./files/static/branding.html, it would be available on your Plumber server at /public/branding.html. You can optionally provide an additional argument to configure the public path used for your server. For instance #* @assets ./files/static /static list() would expose the directory not at /public, but at /static. The “implementation” of your server in the above examples is just an empty list(). You can also specify a function() like you do with the other plumber annotations. At this point, the implementation doesn’t alter the behavior of your static server. Eventually, this list or function may provide an opportunity to configure the server by changing things like cache control settings. If you’re configuring a Plumber router programmatically, you can instantiate a special static file router and mount it onto another router as discussed in the static file router section. 3.6 Input Handling Plumber routes requests based exclusively on the path and method of the incoming HTTP request, but requests can contain much more information than just this. They might include additional HTTP headers, a query string, or a request body. All of these fields may be viewed as “inputs” to your Plumber API. 3.6.1 The Request Object HTTP requests in Plumber are stored as environments and satisfy the Rook interface. The expected objects for all HTTP requests are the following. Name Example Description cookies list(cook=&quot;abc&quot;) A list of the cookies as described in Cookies httpuv.version &quot;1.3.3&quot; The version of the underlying httpuv package PATH_INFO &quot;/&quot; The path of the incoming HTTP request postBody &quot;a=1&amp;b=2&quot; The contents of the body of the request. Despite the name, it is available for any HTTP method. QUERY_STRING &quot;?a=123&amp;b=abc&quot; The query-string portion of the HTTP request REMOTE_ADDR &quot;1.2.3.4&quot; The IP address of the client making the request REMOTE_PORT &quot;62108&quot; The client port from which the request originated REQUEST_METHOD &quot;GET&quot; The method used for this HTTP request rook.errors N/A See Rook docs rook.input N/A See Rook docs rook.url_scheme &quot;http&quot; The “scheme” (typically http or https). rook.version &quot;1.1-0&quot; The version of the rook specification which this environment satisfies SCRIPT_NAME &quot;&quot; Unused SERVER_NAME &quot;127.0.0.1&quot; The host portion of the incoming request. You may favor HTTP_HOST, if available. SERVER_PORT &quot;8000&quot; The target port for the request HTTP_* &quot;HTTP_USER_AGENT&quot; Entries for all of the HTTP headers sent with this request. 3.6.2 Query Strings A query string may be appended to a URL in order to convey additional information beyond just the request route. Query strings allow for the encoding of character string keys and values. For example, in the URL https://duckduckgo.com/?q=bread&amp;pretty=1, everything following the ? constitutes the query string. In this case, two variables (q and pretty) have been set (to bread and 1, respectively). Plumber will automatically forward information from the query string into the function being executed by aligning the name of the query string with the name of the function parameter. The following example defines a search API that mimics the example from DuckDuckGo above but merely prints out what it receives. #&#39; @get / search &lt;- function(q=&quot;&quot;, pretty=0){ paste0(&quot;The q parameter is &#39;&quot;, q, &quot;&#39;. &quot;, &quot;The pretty parameter is &#39;&quot;, pretty, &quot;&#39;.&quot;) } Visiting http://localhost:8000/?q=bread&amp;pretty=1 will print: [&quot;The q parameter is &#39;bread&#39;. The pretty parameter is &#39;1&#39;.&quot;] This is equivalent to calling search(q=&quot;bread&quot;, pretty=&quot;1&quot;). If a parameter were not specified in the query string, it would just be omitted from the invocation of the endpoint. For example http://localhost:8000/?q=cereal would be equivalent to search(q=&quot;cereal&quot;). The function would fall back to the default value of the pretty parameter (0), since that was defined in the function signature. [&quot;The q parameter is &#39;cereal&#39;. The pretty parameter is &#39;0&#39;.&quot;] Including additional query string arguments that do not map to a parameter of the function has no effect. For instance http://localhost:8000/?test=123 will return the same results as calling search(). [&quot;The q parameter is &#39;&#39;. The pretty parameter is &#39;0&#39;.&quot;] (Note that the raw query string is available as req$QUERY_STRING.) Some web browsers impose limitations on the length of a URL. Internet Explorer, in particular, caps the query string at 2,048 characters. If you need to send large amounts of data from a client to your API, it would likely be a better idea to send it in a request body. //TODO: Redundant keys? 3.6.3 Request Body Another way to provide additional information inside an HTTP request is using the message body. Effectively, once a client specifies all the metadata about a request (the path it’s trying to reach, some HTTP headers, etc.) it can then provide a message body. The maximum size of a request body depends largely on the technologies involved (client, proxies, etc.) but is typically at least 2MB – much larger than a query string. This approach is most commonly seen with PUT and POST requests, though you could encounter it with other HTTP methods. Plumber will attempt to parse the request body in one of two ways: if it appears that the message is JSON, then Plumber will parse the body as a JSON message; otherwise it will decode it as a standard query string. Any fields provided in the message body in either format will be passed through as parameters to the function. Unfortunately, crafting a request with a message body requires a bit more work than making a GET request with a query string from your web browser, but you can use tools like curl on the command line or the httr R package. We’ll use curl for the examples below. #&#39; @post /user function(req, id, name){ list( id = id, name = name, raw = req$postBody ) } Running curl --data &quot;id=123&amp;name=Jennifer&quot; &quot;http://localhost:8000/user&quot; will return: { &quot;id&quot;: [123], &quot;name&quot;: [&quot;Jennifer&quot;], &quot;raw&quot;: [&quot;id=123&amp;name=Jennifer&quot;] } Alternatively, curl --data '{&quot;id&quot;:123, &quot;name&quot;: &quot;Jennifer&quot;}' &quot;http://localhost:8000/user&quot; (formatting the body as JSON) will have the same effect. As demonstrated above, the raw request body is made available as req$postBody. 3.6.4 Cookies If cookies are attached to the incoming request, they’ll be made available via req$cookies. This will contain a list of all the cookies that were included with the request. The names of the list correspond to the names of the cookies and the value for each element will be a character string. See the Setting Cookies section for details on how to set cookies from Plumber. If you’ve set encrypted cookies (as discussed in the Encrypted Cookies section), that session will be decrypted and made available at req$session. 3.6.5 Headers HTTP headers attached to the incoming request are attached to the request object. They are prefixed with HTTP_, the name of the header is capitalized, and hyphens are substituted for underscores. e.g. the Content-Type HTTP header can be found as req$HTTP_CONTENT_TYPE. #&#39; Return the value of a custom header #&#39; @get / function(req){ list( val = req$HTTP_CUSTOMHEADER ) } Running curl --header &quot;customheader: abc123&quot; http://localhost:8000 will return: { &quot;val&quot;: [&quot;abc123&quot;] } You can print out the names of all of the properties attached to the request by running print(ls(req)) inside an endpoint. // TODO: Conflicts between path, query string, body? "],
["rendering-and-output.html", "Chapter 4 Rendering Output 4.1 The Response Object 4.2 Serializers 4.3 Error Handling 4.4 Custom Serializers 4.5 Setting Cookies", " Chapter 4 Rendering Output 4.1 The Response Object // TODO 4.2 Serializers In order to send a response from R to an API client, the object must be “serialized” into some format that the client can understand. JavaScript Object Notation (JSON) is one standard which is commonly used by web APIs. JSON serialization translates R objects like list(a=123, b=&quot;hi!&quot;) to JSON text resembling {a: 123, b: &quot;hi!&quot;}. JSON is not appropriate for every situation, however. If you want your API to render an HTML page that might be viewed in a browser, for instance, you will need a different serializer. Likewise, if you want to return an image rendered in R, you likely want to use a standard image format like PNG or JPEG rather than JSON. By default, Plumber serializes objects into JSON via the jsonlite R package. However, there are a variety of other serializers that are built in to the package. Annotation Content Type Description/References @json application/json jsonlite::toJSON() @html text/html; charset=utf-8 Passes response through without any additional serialization @jpeg image/jpeg jpeg() @png image/png png() @htmlwidget text/html; charset=utf-8 htmlwidgets::saveWidget() @unboxedJSON application/json jsonlite::toJSON(unboxed=TRUE) 4.2.1 Bypassing Serialization In some instances it may be desirable to return a value directly from R without serialization. You can bypass serialization by returning the response object from an endpoint. For example, consider the following API. #&#39; Endpoint that bypasses serialization #&#39; @get / function(res){ res$body &lt;- &quot;Literal text here!&quot; res } The response that is returned from this endpoint would contain the body Literal text here! with no Content-Type header and without any additional serialization. Similarly, you can leverage the @serializer contentType annotation which does no serialization of the response but specifies the contentType header. You can use this annotation when you want more control over the response that you send. #* @serializer contentType list(type=&quot;application/pdf&quot;) #* @get /pdf function(){ tmp &lt;- tempfile() pdf(tmp) plot(1:10, type=&quot;b&quot;) text(4, 8, &quot;PDF from plumber!&quot;) text(6, 2, paste(&quot;The time is&quot;, Sys.time())) dev.off() readBin(tmp, &quot;raw&quot;, n=file.info(tmp)$size) } Running this API and visiting http://localhost:8000/pdf will download the PDF generated from R (or display the PDF natively, if your client supports it). 4.2.2 Boxed vs Unboxed JSON You may have noticed that API responses generated from Plumber render singular values (or “scalars”) as arrays. For instance: jsonlite::toJSON(list(a=5)) ## {&quot;a&quot;:[5]} The value of the a element, though it’s singular, is still rendered as an array. This may surprise you initially, but this is done to keep the output consistent. While JSON differentiates scalar from vector objects, R does not. This creates ambiguity when serializing an R object to JSON since it is unclear whether a particular element should be rendered as an atomic value or a JSON array. Consider the following API which returns all the letters lexicographically “higher” than the given letter. #&#39; Get letters after a given letter #&#39; @get /boxed function(letter=&quot;A&quot;){ LETTERS[LETTERS &gt; letter] } #&#39; Get letters after a given letter #&#39; @serializer unboxedJSON #&#39; @get /unboxed function(letter=&quot;A&quot;){ LETTERS[LETTERS &gt; letter] } This is an example of an API that, in some instance, produces a scalar, and in other instances produces a vector. Visiting http://localhost:8000/boxed?letter=U or http://localhost:8000/unboxed?letter=U will return identical responses: [&quot;V&quot;, &quot;W&quot;, &quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;] However, http://localhost:8000/boxed?letter=Y will produce: [&quot;Z&quot;] while http://localhost:8000/unboxed?letter=Y will produce: &quot;Z&quot; The /boxed endpoint, as the name implies, produces “boxed” JSON output in which length-1 vectors are still rendered as an array. Conversely, the /unboxed endpoint sets auto_unbox=TRUE in its call to jsonlite::toJSON, causing length-1 R vectors to be rendered as JSON scalars. While R doesn’t distinguish between scalars and vectors, API clients may respond very differently when encountering a JSON array versus an atomic value. You may find that your API clients will not respond gracefully when an object that they expected to be a vector becomes a scalar in one call. For this reason, Plumber inherits the jsonlite::toJSON default of setting auto_unbox=FALSE which will result in all length-1 vectors still being rendered as JSON arrays. You can configure an endpoint to use the unboxedJSON serializer (as shown above) if you want to alter this behavior for a particular endpoint. There are a couple of functions to be aware of around this feature set. If using boxed JSON serialization, jsonlite::unbox() can be used to force a length-1 object in R to be presented in JSON as a scalar. If using unboxed JSON serialization, I() will cause a length-1 R object to present as a JSON array. 4.3 Error Handling Plumber wraps each endpoint invocation so that it can gracefully capture errors. #&#39; Example of throwing an error #&#39; @get /simple function(){ stop(&quot;I&#39;m an error!&quot;) } #&#39; Generate a friendly error #&#39; @get /friendly function(res){ msg &lt;- &quot;Your request did not include a required parameter.&quot; res$status &lt;- 400 # Bad request list(error=jsonlite::unbox(msg)) } If you run this API and visit http://localhost:8000/simple, you’ll notice two things: An HTTP response with a status code of 500 (“internal server error”) is sent to the client. You should see an error message resembling: {&quot;error&quot;:[&quot;500 - Internal server error&quot;],&quot;message&quot;:[&quot;Error in (function () : I'm an error!\\n&quot;]} A similar error is printed in the terminal where you’re running your Plumber API. This means that it is possible for you to intentionally stop() in an endpoint or a filter as a way to communicate a problem to your user. However, it may be preferable to render errors from your API in a consistent format with more helpful error messages. { &quot;error&quot;: &quot;Your request did not include a required parameter.&quot; } // TODO: example of setErrorHandler on router. 4.4 Custom Serializers // TODO 4.5 Setting Cookies As part of fulfilling a request, a Plumber API can choose to set HTTP cookies on the client. HTTP APIs don’t implicitly contain a notion of a “session.” Without some additional information, Plumber has no way of ascertaining whether or not two HTTP requests that come in are associated with the same user. Cookies offer a way to commission the client to store some state on your behalf so that selected data can outlive a single HTTP request; the full implications of using cookies to track state in your API are discussed here. The two forms of Plumber cookies – plain-text and encrypted – are discussed in the following sections. Before you make cookies an important part of your API’s security model, be sure to understand the section on the security considerations when working with cookies. 4.5.1 Setting Unencrypted Cookies Plumber can both set and receive plaint-text cookies. The API endpoint below will return a random letter, but it remembers your preferences on whether you like capitalized or lower-case letters. #&#39; @put /preferences function(res, capital){ if (missing(capital)){ stop(&quot;You must specify a value for the &#39;capital&#39; preference.&quot;) } res$setCookie(&quot;capitalize&quot;, capital) } #&#39; @get /letter function(req) { capitalize &lt;- req$cookies$capitalize # Default to lower-case unless user preference is capitalized alphabet &lt;- letters # The capitalize cookie will initially be empty (NULL) if (!is.null(capitalize) &amp;&amp; capitalize == &quot;1&quot;){ alphabet &lt;- LETTERS } list( letter = sample(alphabet, 1) ) } Since this API is using a PUT request to test this API, we’ll use curl on the command line to test it. (There’s nothing about cookies that necessitates PUT requests; you could just as easily modify this API to use a GET request.) We can start by visiting the /letter endpoint and we’ll see that the API defaults to a lower-case alphabet. curl http://localhost:8000/letter { &quot;letter&quot;: [&quot;h&quot;] } If we send a PUT request and specify the capital parameter, a cookie will be set on the client which will allow the server to accommodate our preference in future requests. In curl, you need to specify a file in which you want to save these cookies using the -c option. This is a good reminder that clients handle cookies differently – some won’t support them at all – so be sure that the clients you intend to support with your API play nicely with cookies if you want to use them. To send a PUT request, setting the parameter capital to 1, we could invoke: curl -c cookies.txt -X PUT --data 'capital=1' &quot;http://localhost:8000/preferences&quot;. If you print out the cookies.txt file, you should now see that it contains a single cookie called capitalize with a value of 1. We can make another GET request to /letter to see if it accommodates our preferences. But we’ll need to tell curl to use the cookies file we just created when sending this request using the -b switch: curl -b cookies.txt http://localhost:8000/letter. You should now see that the API is returning a random capitalized letter. This is a simple example showing how to persist user preferences across HTTP requests using cookies. But be aware that the client has the ability to modify or fabricate the cookies that they send to your API. So storing preferences that the user themselves provided in a cookie is not a concern. Storing something with security implications like the level of permissions this client has on your API, however, would be; a malicious user would just need to modify the role saved in their cookie in order to trick your API into giving them more permissions than it should. There are two common work-arounds to this concern. You can simply store a long (cryptographically) random identifier in the user’s cookie, and have some mapping on the server that allows you to lookup the session associated with that random ID. Alternatively, you could use signed/encrypted cookies, as detailed in the next section. 4.5.2 Setting Encrypted Cookies In addition to storing plain-text cookies, Plumber also supports handling cookies that are encrypted. Encrypted cookies prevent your users from seeing what is stored inside of them and also sign their contents so that users can’t modify what is stored. To use this feature, you must explicitly add it to your router after constructing it. For example, you could run the following sequence of commands to create a router that supports encrypted session cookies. pr &lt;- plumb(&quot;myfile.R&quot;) pr$registerHooks(sessionCookie(&quot;mySecretHere&quot;, &quot;cookieName&quot;)) pr$run() You’ll notice the above example is using the sessionCookie hooks that come with Plumber. By adding registering these hooks on your router, you’ll ensure that the req$session object is made available on incoming requests and is persisted to the cookie named cookieName when the response is ready to be sent to the user. In this example, the key used to encrypt the data is &quot;mySecretHere&quot;, which is obviously a very weak secret key. Unlike res$setHeader(), the values attached to req$session are serialized via jsonlite; so you’re free to use more complex data structures like lists in your session. Also unlike res$setHeaders(), req$session encrypts the data using the secret key you provide as the first argument to the sessionCookie() function. As an example, we’ll store an encrypted cookie that counts how many times this client has visited a particular endpoint: #* @get /sessionCounter function(req){ count &lt;- 0 if (!is.null(req$session$counter)){ count &lt;- as.numeric(req$session$counter) } req$session$counter &lt;- count + 1 return(paste0(&quot;This is visit #&quot;, count)) } Again, you would need to register the sessionCookie() hooks on your router before this code would work. If you inspect the cookie being set in your browser, you’ll find that its value is encrypted by the time it gets to the client. But by the time it arrives in Plumber, your cookie is available as a regular R list and can be read or modified. "],
["runtime.html", "Chapter 5 Runtime 5.1 Execution Model 5.2 Environments 5.3 Performance &amp; Request Processing 5.4 Managing State", " Chapter 5 Runtime 5.1 Execution Model When you plumb() a file, Plumber calls source() on that file which will evaluate any top-level code that you have defined. # Global code; gets executed at plumb() time. counter &lt;- 0 #&#39; @get / function(){ # Only gets evaluated when this endpoint is requested. counter &lt;&lt;- counter + 1 } If you call plumb() on this file, the counter variable will be created and will live in the environment created for this API. However, the endpoint defined will not be evaluated until it is invoked in response to an incoming request. Because the endpoint uses &lt;&lt;-, the “double-assignment” operator, it mutates the counter variable that was previously defined at plumbtime. This technique allows all endpoints and filters to share some data defined at the top-level of your API. 5.2 Environments By default, when you create a new Plumber router (which happens implicitly when you call plumb() on a file), a new environment is created especially for this router. It is in this environment that all expressions will be evaluated and all endpoints invoked. This can become important if you consider mounting routers onto one another. In that case, you may expect that they would be able to share state via their environment, but that will not work by default. If you’re creating your routers programmatically, then you can specify an environment when initializing your Plumber router using the envir parameter. This is the environment in which: A decorated R script, if provided, will be source()d. All expressions will be evaluated. All endpoint and filter functions will be executed. It is important to be aware that subrouters, by default, would each have their own environment. If you want multiple Plumber routers to share an environment, you will need to provide a single, shared environment when you create the routers. 5.3 Performance &amp; Request Processing R is a single-threaded programming language, meaning that it can only do one task at a time. This is still true when serving APIs using Plumber, so if you have a single endpoint that takes two seconds to generate a response, then every time that endpoint is requested, your R process will be unable to respond to any additional incoming requests for those two seconds. Incoming HTTP requests are serviced in the order in which they appeared, but if requests are coming in more quickly than they can be processed by the API, a backlog of requests will accrue. The common solutions to this problem are to do either or both of: Keep your API performant. All filters and endpoints should complete very quickly and any long-running or complicated tasks should be done outside of the API process. Run multiple R processes to redundantly host a single Plumber API and load-balance incoming requests between all available processes. See the hosting section for details on which hosting environments support this feature. 5.4 Managing State Often, Plumber APIs will require coordination of some state. This state may need to be shared between multiple endpoints in the same API (e.g. a counter that increments every time an endpoint is invoked). Alternatively, it could be information that needs to be persisted across requests from a single client (e.g. storing a preference or setting for some user). Lastly, it might require coordinating between multiple Plumber processes running independently behind a load-balancer. Each of these scenarios have unique properties that determine which solution might be appropriate. As previously discussed, R is single-threaded. Therefore it’s important that you consider the fact that you may eventually need multiple R processes running in parallel to handle the incoming traffic of your API. While this may not seem important initially, you may thank yourself later for designing a “horizontally scalable” API (or one that can be scaled by adding more R processes in parallel). The key to building a horizontally scalable API is to ensure that each Plumber process is “stateless,” meaning that any persistent state lives outside of the Plumber process. In any of the hosting environments that exist today, it is not guaranteed that two subsequent requests from a single client will be served by the same process. Thus it’s never safe to assume that information stored in-memory will be available between requests for a horizontally scaled app. Below are a few options to consider to coordinate state for a Plumber API. 5.4.1 In-Memory As shown previous in the Exetution Model section, it is possible to share state using the environment associated with the Plumber router. # Global code; gets executed at plumb() time. counter &lt;- 0 #&#39; @get / function(){ # Only gets evaluated when this endpoint is requested. counter &lt;&lt;- counter + 1 } This is the one approach presented that does not allow your Plumber process to be stateless. The approach is sufficient for coordinating state within a single process, but as you scale your API by adding processes, this state will no longer be coordinated between them. Therefore this approach can be effective for “read-only” data – such as if you were to load a single, large dataset into memory when the API starts, then allow all filters and endpoints to reference that dataset moving forward – but it will not allow you to share state across multiple processes as you scale. If you want to build a scalable, stateless application, you should avoid relying on the in-memory R environment to coordinate state between the pieces of your API. 5.4.2 File System Writing to files on disk is often the next most obvious choice for storing state. Plumber APIs could modify a data frame then use write.csv() to save that data to disk, or use writeLines() to append some new data to an existing file. These approaches enable your R process to be stateless, but are not always resilient to concurrency issues. For instance, if you’ve horizontally scaled your API to five R processes and two go to write.csv() simultaneously, you will either see one process’s data get immediately overwritten by the other’s, or – even worse – you may end up with a corrupted CSV file which can’t be read. Unless otherwise stated, it’s safe to assume that any R function that writes data to disk is not resilient to concurrency contention, so you should not rely on the filesystem to coordinate shared state for any more than a single R process running concurrently. It’s also important to ask whether or not the hosting platform you’ll be using supports persistent storage on disk. For instance, Docker may insulate your R process from your hardware and not allow you to write outside of your container. RStudio Connect, too, will provision a new directory every time you deploy an updated version of your API which will discard any data you had written to disk up to that point. So if you’re considering writing your state to disk long-term, be sure that your hosting environment supports persistent on-disk storage and that you’ve considered the concurrency implications of your code. 5.4.3 Cookies HTTP cookies are a convention that allow web servers to send some state to a client with the expectation that the client would then include that state in future requests. See the Setting Cookies section for details on how to leverage cookies in Plumber. // TODO: graphic All modern web browsers support cookies (unless configured not to) and many other clients do, as well, though some clients require additional configuration in order to do so. If you’re confident that the intended clients for your API support cookies then you could consider storing some state in cookies. This approach mitigates concerns about horizontal scalability, as the state is written to each client independently and then included in subsequent requests from that client. This also minimizes the infrastructure requirements for hosting your Plumber APIs since you don’t need to setup a system capable of storing all of this state; instead, you’ve commissioned your clients to store their own state. One issue with maintaining state in cookies is that their size should be kept to a minimum. Clients impose restrictions differently, but you should not plan to store more than 4kB of information in a cookie. And realize that whatever information gets placed in the cookie must be retransmitted by the client with every request. This can significantly increase the size of each HTTP request that your clients make. The most notable concern when considering using cookies to store state is that since your clients are responsible for storing and sending their state, you cannot expect that the state has not been tampered with. Thus, while it may be acceptable to store user preferences like preferredColor=&quot;blue&quot;, you should not store authentication information like userID=1493, since the user could trivially change that cookie to another user’s ID to impersonate them. If you’d like to use cookies to store information with guarantees that the user cannot either read or modify the state, see the Encrypted Cookies section). 5.4.4 External Data Store The final option to consider when coordinating state for an API is leveraging an external data store. This could be a relational database (like MySQL or Amazon RedShift), a non-relational database (like MongoDB), or an transactional data store like Redis. One important consideration for any of these options is to ensure that they are “transactional,” meaning that two Plumber processes trying to write at the same time won’t overwrite one another. If you’re interested in pursuing this option you should see db.rstudio.com or looks at some of the resources put together for Shiny as pertains to dealing with databases in a web-accessible R platform. "],
["security.html", "Chapter 6 Security 6.1 Networking &amp; Firewalls 6.2 Denial Of Service (DoS) 6.3 Sanitization &amp; Injection 6.4 Cross-Origin Resource Sharing (CORS) 6.5 Cross-Site Request Forgery (CSRF, or XSRF) 6.6 Cookies", " Chapter 6 Security The majority of R programmers have not been trained to give much attention to the security of the code that they write. This is for good reason since running R code on your own machine with no external input gives little opportunity for attackers to leverage your R code to do anything malicious. However, as soon as you expose an API on a network, your concerns and thought process must adapt accordingly. There are a variety of factors and attacks that you should consider when developing your Plumber APIs, but the attention you give to each will vary based on the audience of your API. On one extreme, if your API is running on an internal server on your own network and uses authentication to restrict access to only a handful of trusted users, you may be justified in overlooking some of these attack vectors. On the other hand, if your Plumber API is available on the Internet with no authentication, then you should seriously consider each of these potential vulnerabilities and convince yourself that you have properly accounted for each. 6.1 Networking &amp; Firewalls From a networking standpoint, there are two fundamentally different approaches for developing R code. You can develop locally using a tool like RStudio Desktop. In this case, the R session (and any Plumber APIs that you run()) will be housed on your local machine. You can develop on a remote machine using a tool like RStudio Server. Here, the R session is running on a remote server accessed across a network. In the first case, there’s typically very little to consider from a networking perspective. Your APIs will be accessible at http://127.0.0.1:8000 by default (localhost is synonymous with the local IP address 127.0.0.1) and you likely won’t need to concern yourself with firewalls or network proxies. In the second case, however, you may need to consider the network environment in between yourself and the server running the API. These are the same considerations you’ll need to make when hosting an API on a server for production use. In particular, you should investigate whether or not there are any firewalls between the server hosting the Plumber API and the clients that you want to be able to connect. Firewalls are a way to block undesired network traffic. Most desktop computers and many servers come with firewalls enabled out-of-the-box. This means that if you want to expose your API running on port 8000, you will need to configure your firewall to accept incoming connections on that port. Firewalls can also be configured on other network intermediaries, so you may need to configure multiple firewalls to allow traffic through in order to expose the desired port to your API clients. 6.2 Denial Of Service (DoS) Denial of service (DoS) attacks are employed in order to temporarily shut down a server or service by overwhelming it with traffic. A DoS scenario could be caused by a single ignorant user unintentionally making a request that could ask the server to do some impossible task, or could be intentionally introduced by a malicious actor leveraging a vast number of machines to repeatedly make requests that are expensive for the server to respond to. The later form is often called a distributed denial of service attack (or DDoS) and typically requires special infrastructure and network capacity that is beyond the scope of what we’ll discuss here. However, there are practices that you should employ when designing your Plumber API to put safety guards around the work that an API request might instigate. #&#39; This is an example of an UNSAFE endpoint which #&#39; is vulnerable to a DOS attack. #&#39; @get / #&#39; @png function(pts=10) { # An example of an UNSAFE endpoint. plot(1:pts) } The expected output here is a harmless plot. This plot takes a negligible amount of time to create. However, plots with more points will take more time to create. This plot, with 10,000 points added, took 0.538 seconds to generate. While that doesn’t sound like much, if we exposed this API publicly on the Internet, an attacker could easily generate enough traffic on this endpoint to overwhelm the Plumber process. Even worse, an attacker could make a request on this endpoint with millions or billions of points which might cause our server to run out of memory or consume so much CPU that it deprives other important system resources. Either case could result in our Plumber process crashing altogether. The solution, in this case, is to ensure that we have reasonable safety guards in place for any user input. #&#39; This is an example of an safe endpoint which #&#39; checks user input to avoid a DOS attack #&#39; @get / #&#39; @png function(pts=10) { if (pts &gt; 1000){ stop(&quot;pts must be &lt; 1,000&quot;) } plot(1:pts) } Here you can see that we only allow the user to request a graph with up to 1,000 points. Any requests exceeding that limit will immediately be terminated without any further computation. You should be very attentive to the resources that could be consumed by any of your filters or endpoints. Consider the various values that a user could provide for each of your API endpoint’s parameters and ensure that the behavior of the system is reasonable in those cases. For API endpoints that do require extensive computation, consider how you could protect those endpoints (perhaps only exposing them for authenticated users) to prevent a malicious user from abusing the system. 6.3 Sanitization &amp; Injection Any time you accept input from a user in your code, you should plan for the worst-case scenario. If, for example, your API endpoint allows the user to specify the name of a file that should be read out of a particular directory and then returns its contents, you might naively implement it like so. #&#39; This is an example of an UNSAFE endpoint which #&#39; does not sanitize user input #&#39; @get / function(file) { # An example of an UNSAFE endpoint. path &lt;- file.path(&quot;./datasets&quot;, file) readLines(path) } Unfortunately, this API endpoint does not properly sanitize the user input. The user could set the file parameter to ../plumber.R and now the endpoint would return the source code of your Plumber API. Of course they could just as easy attempt to read other files that might contain API keys or other sensitive data. One solution in this case is to strip all special characters off of the user input which will prevent users from being able to escape into a different directory. #&#39; This is an example of an endpoint which #&#39; checks user input. #&#39; @get / function(file) { # Strip all &quot;non-word&quot; characters from user input sanitizedFile &lt;- gsub(&quot;\\\\W&quot;, &quot;&quot;, file) path &lt;- file.path(&quot;./datasets&quot;, sanitizedFile) readLines(path) } File paths are not the opportunity for malicious input to do damage to your system, however. Another way in which user input can be dangerous is an attack known as “cross site scripting,” or “XSS.” This attack can be leveraged whenever user input may be rendered in a user’s browser. For instance if you had an endpoint which allows users to comment on a page and then later displays those comments to other users, an attacked could craft a comment such as: &quot;This is a comment with JavaScript! &lt;script&gt;alert(&#39;I could do something bad here!&#39;);&lt;/script&gt; As you can see, the comment has JavaScript embedded within it, in this case used to popup a message to the user. Of course JavaScript could be used in other harmful way by redirecting your users to a malicious site, for instance, or uploading data that they have special access to on your server to some other destination. Any user input that might be included on an HTML page should be properly escaped (see htmltools::html_escape for help). Lastly, user input can be used in an “injection attack,” in which the user injects malicious commands that might be sent to another system. The best known in this family are SQL injection attacks, in which user input that is meant to be included in a SQL query to be executed against a database might contain additional SQL commands that could leak data or do damage to your database. Further details about SQL injection attacks and mitigation strategies in R are available here. In summary, be sure to separate “trusted” from “untrusted” objects in your API implementation. Anything which the user provides should be considered “untrusted” until it has been escaped or sanitized. At that point you can consider the object to be “trusted” and proceed to take further actions on it. 6.4 Cross-Origin Resource Sharing (CORS) // TODO CORS 6.5 Cross-Site Request Forgery (CSRF, or XSRF) // TODO XSRF 6.6 Cookies // TODO: Encrypted cookies can’t be invalidated server-side // TODO: HTTP-only cookies // TODO: Secure cookies // TODO: cookies with regards to XSRF // TODO: stolen cookies can be used to impersonate. "],
["hosting.html", "Chapter 7 Hosting 7.1 DigitalOcean 7.2 RStudio Connect 7.3 Docker (Basic) 7.4 Docker (Advanced) 7.5 pm2", " Chapter 7 Hosting Once you have developed your Plumber API, the next step is to find a way to host it. If you haven’t dealt with hosting an application on a server before, you may be tempted to run the run() command from an interactive session on your development machine (either your personal desktop or an RStudio Server instance) and direct traffic there. This is a dangerous idea for a number of reasons: Your development machine likely has a dynamic IP address. This means that clients may be able to reach you at that address today, but it will likely break on you in the coming weeks/months. Networks may leverage firewalls to block incoming traffic to certain networks and machines. Again, it may appear that everything is working for you locally, but other users elsewhere in the network or external clients may not be able to connect to your development machine. If your Plumber process crashes (for instance, due to your server running out of memory), the method of running Plumber will not automatically restart the crashed service for you. This means that your API will be offline until you manually login and restart it. Likewise if your development machine gets rebooted, your API will not automatically be started when the machine comes back online. This technique relies on having your clients specify a port number manually. Non-technical users may be tripped up by this; some of the other techniques do not require clients specifying the port for an API. This approach will eternally run one R process for your API. Some of the other approaches will allow you to load-balance traffic between multiple R processes to handle more requests. RStudio Connect will even dynamically scale the number of running processes for you so that your API isn’t consuming more system resources than is necessary. Plumber uses the interactive() function as a heuristic for whether it should behave in a more convenient mode or in a more robust, secure mode. This function is used as the default for some parameters on run() which could pose a security hazard if enabled on a public server. Most importantly, serving public requests from your development environment can be a security hazard. Ideally, you should separate your development instances from the servers that are accessible by others. For these reasons and more, you should consider setting up a separate server on which you can host your Plumber APIs. There are a variety of options that you can consider. 7.1 DigitalOcean DigitalOcean is an easy-to-use Cloud Computing provider. They offer a simple way to spin up a Linux virtual machine and access it remotely. You can choose what size machine you want to run – with options ranging from small machines with 512MB of RAM for a few dollars a month up to large machines with dozens of GB of RAM – and only pay for it while it’s online. Plumber includes helper functions that enable you to automatically provision a Plumber server and deploy your APIs to it. So in order to setup a Plumber server running on DigitalOcean, you’ll follow these steps: Create a DigitalOcean account. Setup an SSH key and deploy the public portion to DigitalOcean so you’ll be able to login to your server. Install the analogsea R package and run a test command like analogsea::droplets() to confirm that it’s able to connect to your DigitalOcean account. Run mydrop &lt;- plumber::do_provision(). This will start a virtual machine (or “droplet”, as DigitalOcean calls them) and install Plumber and all the necessary prerequisite software. Once the provisioning is complete, you should be able to access port 8000 on your server’s IP and see a response from Plumber. Install any R packages on the server that your API requires using analogsea::install_r_package(). You can use plumber::do_deploy_api() to deploy or update your own custom APIs to a particular port on your server. (Optional) Setup a domain name for your Plumber server so you can use www.myplumberserver.com instead of the server’s IP address. (Optional) Configure SSL Getting everything connected the first time can be a bit of work, but once you have analogsea connected to your DigitalOcean account, you’re not able to spin up new Plumber servers in DigitalOcean hosting your APIs with just a couple of R commands. You can even write scripts that provision an entire Plumber server with multiple APIs associated. 7.2 RStudio Connect RStudio Connect is an enterprise publishing platform from RStudio. It supports push-button publishing from the RStudio IDE of a variety of R content types including Plumber APIs. Unlike all the other options listed here, RStudio Connect automatically manages the dependent packages and files your API has and recreates an environment closely mimicking your local development environment on the server. RStudio Connect automatically manages the number of R processes necessary to handle the current load and balances incoming traffic across all available processes. It can also shut down idle processes when they’re not in use. This allows you to run the appropriate number of R processes to scale your capacity to accommodate the current load. Conflict of interest: the primary author of plumber and this book works for RStudio on RStudio Connect. 7.3 Docker (Basic) Docker is a platform built on top of Linux Containers that allow you to run processes in an isolated environment; that environment might have certain resources/software pre-configured or may emulate a particular Linux environment like Ubuntu 14.04 or CentOS 7.3. We won’t delve into the details of Docker or how to setup or install everything on your system. Docker provides some great resources for those who are looking to get started. Here we’ll assume that you have Docker installed and you’re familiar with the basic commands required to spin up a container. In this article, we’ll take advantage of the trestletech/plumber Docker image that bundles a recent version of R with the most recent version of plumber pre-installed (the underlying R image is courtesy of the rocker project). You can get this image with a docker pull trestletech/plumber Remember that this will get you the current snapshot of Plumber and will continue to use that image until you run pull again. 7.3.1 Default Dockerfile We’ll start by just running a single Plumber application in Docker just to see things at work. By default, the trestletech/plumber image will take the first argument after the image name as the name of the file that you want to plumb() and serve on port 8000. So right away you can run one of the examples that’s included in plumber as it is already installed on the image. docker run --rm -p 8000:8000 trestletech/plumber which is the same as: docker run --rm -p 8000:8000 trestletech/plumber \\ /usr/local/lib/R/site-library/plumber/examples/04-mean-sum/plumber.R docker run tells Docker to run a new container --rm tells Docker to clean-up after the container when it’s done -p 8000:8000 says to map port 8000 from the plumber container (which is where we’ll run the server) to port 8000 of your local machine trestletech/plumber is the name of the image we want to run /usr/local/lib/R/site-library/plumber/examples/03-mean-sum/plumber.R is the path inside of the Docker container to the Plumber file you want to host. You’ll note that you do not need plumber installed on your host machine for this to work, nor does the path /usr/local/... need to exist on your host machine. This references the path inside of the docker container where the R file you want to plumb() can be found. This mean-sum path is the default path that the image uses if you don’t specify one yourself. This will ask Plumber to plumb and run the file you specified on port 8000 of that new container. Because you used the -p argument, port 8000 of your local machine will be forwarded into your container. You can test this by running this on the machine where Docker is running: curl localhost:8000/mean, or if you know the IP address of the machine where Docker is running, you could visit it in a web browser. The /mean path is one that’s defined in the plumber file we just specified – you should get an single number in an array back ([-0.1993]). If that works, you can try using one of your own plumber files in this arrangement. Keep in mind that the file you want to run must be available inside of the container and you must specify the path to that file as it exists inside of the container. Keep it simple for now – use a plumber file that doesn’t require any additional R packages or depend on any other files outside of the plumber definition. For instance if you have a plumber file saved in your current directory called api.R, you could use the following command docker run --rm -p 8000:8000 -v `pwd`/api.R:/plumber.R trestletech/plumber /plumber.R You’ll notice that we used the -v argument to specify a “volume” that should be mapped from our host machine into the Docker container. We defined that the location of that file should be at /plumber.R, so that’s the argument we give last to tell the container where to look for the plumber definition. You can use this same technique to share a whole directory instead of just passing in a single R file; this approach is useful if your Plumber API depends on other files. You can also use the trestletech/plumber image just like you use any other. For example, if you want to start a container based on this image and poke around in a bash shell: docker run -it --rm --entrypoint /bin/bash trestletech/plumber This can be a handy way to debug problems. Prepare the command that you think should work then add --entrypoint /bin/bash before trestletech/plumber and explore a bit. Alternatively, you can try to run the R process and spawn the plumber application yourself and see where things go wrong (often a missing package or missing file). 7.3.2 Custom Dockerfiles You can build upon the trestletech/plumber image and build your own Docker image by writing your own Dockerfile. Dockerfiles have a vast array of options and possible configurations, so see the official docs if you want to learn more about any of these options. A couple of commands that are relevant here: RUN runs a command and persists the side-effects in the Docker image you’re building. So if you want to build a new image that has the broom package, you could add a line in your Dockerfile that says RUN R -e &quot;install.packages('broom')&quot; which would make the broom package available in your new Docker image. ENTRYPOINT is the command to run when starting the image. trestletech/plumber specifies an entrypoint that starts R, plumb()s a file, then run()s the router. If you want to change how plumber starts, or run some extra commands (like add a global processor) before you run the router, you’ll need to provide a custom ENTRYPOINT. CMD these are the default arguments to provide to ENTRYPOINT. trestletech/plumber uses only the first argument as the name of the file that you want to plumb(). So your custom Dockerfile could be as simple as: FROM trestletech/plumber MAINTAINER Docker User &lt;docker@user.org&gt; RUN R -e &quot;install.packages(&#39;broom&#39;)&quot; CMD [&quot;/app/plumber.R&quot;] This Dockerfile would just extend the trestletech/plumber image in two ways. First, it RUNs one additional command to install the broom package. Second, it customizes the default CMD argument that will be used when running the image. In this case, you would be expected to mount a Plumber application into the container at /app/plumber.R You could then build your custom Docker image from this Dockerfile using the command docker build -t myCustomDocker . (where . – the current directory – is the directory where that Dockerfile is stored). Then you’d be able to use docker run --rm -vpwd:/app myCustomDocker to run your custom image, passing in your application’s directory as a volume mounted at /app. 7.3.3 Automatically Run on Restart If you want your container to start automatically when your machine is booted, you can use the -d switch for docker run. docker run -p 1234:8000 -d myCustomDocker would run the custom image you created above automatically every time your machine boots and expose the plumber service on port 1234 of your host machine. Like all other hosting options, you’ll need to make sure that your firewall allows connections on port 1234 if you want others to be able to access your service. 7.4 Docker (Advanced) If you already have a basic Docker instance running, you may be interested in more advanced configurations capable of hosting multiple plumber applications on a single server and even load-balancing across multiple plumber processes. In order to coordinate and run multiple Plumber processes on one server, you should install docker-compose on your system. This is not included with some installations of Docker, so you will need to follow these instructions if you are not currently able to run docker-compose on the command-line. Docker Compose helps orchestrate multiple Docker containers. If you’re planning to run more than one Plumber process, you’ll want to use Docker Compose to keep them all alive and route traffic between them. 7.4.1 Multiple Plumber Applications We’ll use Docker Compose to help us organize multiple Plumber processes. We won’t go into detail about how to use Docker Compose, so if you’re new you should familiarize yourself using the official docs. You should define a Docker Compose configuration that defines the behavior of every Plumber application that you want to run. You’ll first want to setup a Dockerfile that defines the desired behavior for each of your applications (as we outlined previously. You could use a docker-compose.yml configuration like the following: version: &#39;2&#39; services: app1: build: ./app1/ volumes: - ./data:/data - ./app1:/app restart: always ports: - &quot;7000:8000&quot; app2: image: trestletech/plumber command: /app/plumber.R volumes: - ../app2:/app restart: always ports: - &quot;7001:8000&quot; More detail on what each of these options does and what other options exist can be found here. This configuration defines two Docker containers that should run app1 and app2. The associated files in this case are layed out on disk as follows: docker-compose.yml app1 ├── Dockerfile ├── api.R app2 ├── plumber.R data ├── data.csv You can see that app2 is the simpler of the two apps; it just has the plumber definition that should be run through plumb(). So we merely use the default plumber Docker image as its image, and then customize the command to specify where the Plumber API definition can be found in the container. Since we’re mapping our host’s ./app2 to /app inside of the container, the definition would be found in /app/plumber.R. We specify that it should always restart if anything ever happens to the container, and we export port 8000 from the container to port 7001 on the host. app1 is our more complicated app. It has some extra data in another directory that needs to be loaded, and it has a custom Dockerfile. This could be because it has additional R packages or system dependencies that it requires. If you now run docker-compose up, Docker Compose will build the referenced images in your config file and then run them. You’ll find that app1 is available on port 7000 of the machine running Docker Compose, and app2 is available on port 7001. If you want these APIs to run in the background and survive restarts of your server, you can use the -d switch just like with docker run. 7.4.2 Multiple Applications on One Port It may desirable to run all of your Plumber services on a standard port like 80 (for HTTP) or 443 (for HTTPS). In that case, you’d prefer to have a router running on port 80 that can send traffic to the appropriate Plumber API by distinguishing based on a path prefix. Requests for myserver.com/app1/ could be sent to the app1 container, and myserver.org/app2/ could target the app2 container, but both paths would be available on port 80 on your server. In order to do this, we can use another Docker container running nginx which is configured to route traffic between the two Plumber containers. We’d add the following entry to our docker-compose.yml below the app containers we already have defined. nginx: image: nginx:1.9 ports: - &quot;80:80&quot; volumes: - ./nginx.conf:/etc/nginx/nginx.conf:ro restart: always depends_on: - app1 - app2 This uses the nginx Docker image that will be downloaded for you. In order to run nginx in a meaningful way, we have to provide a configuration file and place it in /etc/nginx/nginx.conf, which we do by mounting a local file at that location on the container. A basic nginx config file could look something like the following: events { worker_connections 4096; ## Default: 1024 } http { default_type application/octet-stream; sendfile on; tcp_nopush on; server_names_hash_bucket_size 128; # this seems to be required for some vhosts server { listen 80 default_server; listen [::]:80 default_server ipv6only=on; root /usr/share/nginx/html; index index.html index.htm; server_name MYSERVER.ORG location /app1/ { proxy_pass http://app1:8000/; proxy_set_header Host $host; } location /app2/ { proxy_pass http://app2:8000/; proxy_set_header Host $host; } location ~ /\\.ht { deny all; } } } You should set the server_name parameter above to be whatever the public address is of your server. You can save this file as nginx.conf in the same directory as your Compose config file. Docker Compose is intelligent enough to know to route traffic for http://app1:8000/ to the app1 container, port 8000, so we can leverage that in our config file. Docker containers are able to contact each other on their non-public ports, so we can go directly to port 8000 for both containers. This proxy configuration will trim the prefix off of the request before it sends it on to the applications, so your applications don’t need to know anything about being hosted publicly at a URL that includes the /app1/ or /app2/ prefixes. We should also get rid of the previous port mappings to ports 7000 and 7001 on our other applications, as we don’t want to expose our APIs on those ports anymore. If you now run docker compose up again, you’ll see your two application servers running but now have a new nginx server running, as well. And you’ll find that if you visit your server on port 80, you’ll see the “welcome to Nginx!” page. If you access /app1 you’ll be sent to app1 just like we had hoped. 7.4.3 Load Balancing If you’re expecting a lot of traffic on one application or have an API that’s particularly computationally complex, you may want to distribute the load across multiple R processes running the same Plumber application. Thankfully, we can use Docker Compose for this, as well. First, we’ll want to create multiple instances of the same application. This is easily accomplished with the docker-compose scale command. You simply run docker-compose scale app1=3 to run three instances of app1. Now we just need to load balance traffic across these three instances. You could setup the nginx configuration that we already have to balance traffic across this pool of workers, but you would need to manually re-configure and update your nginx instance every time that you need to scale the number up or down, which might be a hassle. Luckily, there’s a more elegant solution. We can use the dockercloud/haproxy Docker image to automatically balance HTTP traffic across a pool of workers. This image is intelligent enough to listen for workers in your pool arriving or leaving and will automatically remove/add these containers into their pool. Let’s add a new container into our configuration that defines this load balancer lb: image: &#39;dockercloud/haproxy:1.2.1&#39; links: - app1 volumes: - /var/run/docker.sock:/var/run/docker.sock The trick that allows this image to listen in to our scaling of app1 is by passing in the docker socket as a shared volume. Note that this particular arrangement will differ based on your host OS. The above configuration is intended for Linux, but MacOS X users would require a (slightly different config](https://github.com/docker/dockercloud-haproxy#example-of-docker-composeyml-running-in-linux). We could export port 80 of our new load balancer to port 80 of our host machine if we solely wanted to load-balance a single application. Alternatively, we can actually use both nginx (to handle the routing of various applications) and HAProxy (to handle the load balancing of a particular application). To do that, we’d merely add a new location block to our nginx.conf file that knows how to send traffic to HAproxy, or modify the existing location block to send traffic to the load balancer instead of going directly to the application. So the location /app1/ block becomes: location /app1/ { proxy_pass http://lb:8000/; proxy_set_header Host $host; } Where lb is the name of the HAProxy load balancer that we defined in our Compose configuration. The next time you start/redeploy your Docker Compose cluster, you’ll be balancing your incoming requests to /app1/ across a pool of 1 or more R processes based on whatever you’ve set the scale to be for that application. Do keep in mind that when using load-balancing that it’s not longer guaranteed that subsequent requests for a particular application will land on the same process. This means that if you maintain any state in your Plumber application (like a global counter, or a user’s session state), you can’t expect that to be shared across the processes that the user might encounter. There are at least three possible solutions to this problem: Use a more robust means of maintaing state. You could put the state in a database, for instance, that lives outside of your R processes and your Plumber processes could get and save their state externally. You could serialize the state to the user using (encrypted) session cookies, assuming it’s small enough. In this scenario, your workers would write data back to the user in the form of a cookie, then the user would include that same cookie in its subsequent requests. This works best if the state is going to be set rarely and read often (for instance, the cookie could be set when the user logs in, then read on each request to detect the identity of this user). You can enable “sticky sessions” in the HAProxy load balancer. This would ensure that each user’s traffic always gets routed to the same worker. The downside of this approach is that it will distribute traffic less evenly. You could end up in a situation in which you have 2 R processes for an application but 90% of your traffic is hitting one of them if it happens the users triggering the majority of the requests are all “stuck” to one particular worker. 7.5 pm2 If you don’t have the luxury of running your Plumber instance on a designated server (as is discussed in the DigitalOcean section) and you’re not comfortable hosting the API in Docker, then you’ll need to find a way to run and manage your Plumber APIs on your server directly. There are a variety of tools that were built to help manage web hosting in a single-threaded environment like R. Some of the most compelling tools were developed around Ruby (like Phusion Passenger) or Node.js (like Node Supervisor, forever or pm2). Thankfully, many of these tools can be adapted to support managing an R process running a Plumber API. pm2 is a process manager initially targeting Node.js. Here we’ll show the commands needed to do this in Ubuntu 14.04, but you can use any Operating System or distribution that is supported by pm2. At the end, you’ll have a server that automatically starts your plumber services when booted, restarts them if they ever crash, and even centralizes the logs for your plumber services. 7.5.1 Server Deployment and Preparation The first thing you’ll need to do, regardless of which process manager you choose, is to deploy the R files containing your plumber applications to the server where they’ll be hosted. Keep in mind that you’ll also need to include any supplemental R files that are source()d in your plumber file, and any other datasets or dependencies that your files have. You’ll also need to make sure that the R packages you need (and the appropriate versions) are available on the remote server. You can either do this manually by installing those packages or you can consider using a tool like Packrat to help with this. There are a myriad of features in pm2 that we won’t cover here. It is a good idea to spend some time reading through their documentation to see which features might be of interest to you and to ensure that you understand all the implications of how pm2 hosts services (which user you want to run your processes as, etc.). Their quick-start guide may be especially relevant. For the sake of simplicity, we will do a basic installation here without customizing many of those options. 7.5.2 Install pm2 Now you’re ready to install pm2. pm2 is a package that’s maintained in npm (Node.js’s package management system); it also requires Node.js in order to run. So to start you’ll want to install Node.js. On Ubuntu 14.04, the necessary commands are: sudo apt-get update sudo apt-get install nodejs npm Once you have npm and Node.js installed, you’re ready to install pm2. sudo npm install -g pm2 This will install pm2 globally (-g) on your server, meaning you should now be able to run pm2 --version and get the version number of pm2 that you’ve installed. In order to get pm2 to startup your services on boot, you should run sudo pm2 startup which will create the necessary files for your system to run pm2 when you boot your machine. 7.5.3 Wrap Your Plumber File Once you’ve deployed your Plumber files onto the server, you’ll still need to tell the server how to run your server. You’re probably used to running commands like pr &lt;- plumb(&quot;myfile.R&quot;) pr$run(port=4500) Unfortunately, pm2 doesn’t understand R scripts natively; however, it is possible to specify a custom interpreter. We can use this feature to launch an R-based wrapper for our plumber file using the Rscript scripting front-end that comes with R. The following script will run the two commands listed above. #!/usr/bin/env Rscript library(plumber) pr &lt;- plumb(&#39;myfile.R&#39;) pr$run(port=4000) Save this R script on your server as something like run-myfile.R. You should also make it executable by changing the permissions on the file using a command like chmod 755 run-myfile.R. You should now execute that file to make sure that it runs the service like you expect. You should be able to make requests to your server on the appropriate port and have the plumber service respond. You can kill the process using Ctrl-c when you’re convinced that it’s working. Make sure the shell script is in a permanent location so that it won’t be erased or modified accidentally. You can consider creating a designated directory for all your plumber services in some directory like /usr/local/plumber, then put all services and their associated Rscript-runners in their own subdirectory like /usr/local/plumber/myfile/. 7.5.4 Introduce Our Service to pm2 We’ll now need to teach pm2 about our Plumber API so that we can put it to work. You can register and configure any number of services with pm2; let’s start with our myfile Plumber service. You can use the pm2 list command to see which services pm2 is already running. If you run this command now, you’ll see that pm2 doesn’t have any services that it’s in charge of. Once you have the scripts and code stored in the directory where you want them, use the following command to tell pm2 about your service. pm2 start --interpreter=&quot;Rscript&quot; /usr/local/plumber/myfile/run-myfile.R You should see some output about pm2 starting an instance of your service, followed by some status information from pm2. If everything worked properly, you’ll see that your new service has been registered and is running. You can see this same output by executing pm2 list again. Once you’re happy with the pm2 services you have defined, you can use pm2 save to tell pm2 to retain the set of services you have running next time you boot the machine. All of the services you have defined will be automatically restarted for you. At this point, you have a persistent pm2 service created for your Plumber application. This means that you can reboot your server, or find and kill the underlying R process that your plumber application is using and pm2 will automatically bring a new process in to replace it. This should help guarantee that you always have a Plumber process running on the port number you specified in the shell script. It is a good idea to reboot the server to ensure that everything comes back the way you expected. You can repeat this process with all the plumber applications you want to deploy, as long as you give each a unique port to run on. Remember that you can’t have more than one service running on a single port. And be sure to pm2 save every time you add services that you want to survive a restart. 7.5.5 Logs and Management Now that you have your applications defined in pm2, you may want to drill down into them to manage or debug them. If you want to see more information, use the pm2 show command and specify the name of the application from pm2 list. This is usually the same as the name of the shell script you specified, so it may be something like pm2 show run-myfile. You can peruse this information but keep an eye on the restarts count for your applications. If your application has had to restart many times, that implies that the process is crashing often, which is a sign that there’s a problem in your code. Thankfully, pm2 automatically manages the log files from your underlying processes. If you ever need to check the log files of a service, you can just run pm2 logs run-myfile, where myfile is again the name of the service obtained from pm2 list. This command will show you the last few lines logged from your process, and then begin streaming any incoming log lines until you exit (Ctrl-c). If you want a big-picture view of the health of your server and all the pm2 services, you can run pm2 monit which will show you a dashboard of the RAM and CPU usage of all your services. "],
["programmatic-usage.html", "Chapter 8 Programmatic Usage 8.1 Creating and Controlling a Router 8.2 Defining Endpoints 8.3 Defining Filters 8.4 Registering Hooks on a Router {router-hooks} 8.5 Mounting &amp; Static File Routers 8.6 Customizing a Router", " Chapter 8 Programmatic Usage Typically, users define APIs using the “annotations,” or special comments in their API source code. It is possible to define a Plumber API programmatically using the same underlying R6 objects that get created automatically when you define your API using annotations. Interacting with Plumber at this level can offer more control and specificity about how you want your API to behave. 8.1 Creating and Controlling a Router The centerpiece of Plumber is the router. Plumber routers are responsible for coordinating incoming requests from httpuv, dispatching the requests to the appropriate filters/endpoints, serializing the response, and handling errors that might pop up along the way. If you’ve been using annotations to define Plumber APIs, then you’ve already worked with Plumber routers as that’s what the plumb() command produces. To instantiate a new Plumber router programmatically, you can call plumber$new(). This will return a blank Plumber router with no endpoints. You could call run() on the returned object to start the API, but it doesn’t know how to respond to any requests so any incoming traffic would get a 404 response. We’ll see momentarily how to add endpoints and filters onto this empty router. Alternatively, you can pass a file that contains your annotation-based Plumber API as the first parameter to create a router much like you do with plumb(). Be aware that Plumber routers do come with a handful of filters pre-configured. These built-in filters are used to do things like process properties of the incoming request like its cookies, POST body, or query string. You can specify which filters you want in your new router by overriding the filters parameter when creating your new router. 8.2 Defining Endpoints You can define endpoints on your router by using the handle() method. For instance, to define a Plumber API that response to GET requests on / and POST requests on /submit, you could use the following code: pr &lt;- plumber$new() pr$handle(&quot;GET&quot;, &quot;/&quot;, function(req, res){ # ... }) pr$handle(&quot;POST&quot;, &quot;/submit&quot;, function(req, res){ # ... }) The “handler” functions that you define in these handle calls are identical to the code you would have defined in your plumber.R file if you were using annotations to define your API. The handle() method takes additional arguments that allow you to control nuanced behavior of the endpoint like which filter it might preempt or which serializer it should use. For instance, the following endpoint would use Plumber’s HTML serializer. pr &lt;- plumber$new() pr$handle(&quot;GET&quot;, &quot;/&quot;, function(){ &quot;&lt;html&gt;&lt;h1&gt;Programmatic Plumber!&lt;/h1&gt;&lt;/html&gt;&quot; }, serializer=plumber::serializer_html()) 8.3 Defining Filters Use the filter() method of a Plumber router to define a new filter: pr &lt;- plumber$new() pr$filter(&quot;myFilter&quot;, function(req){ req$filtered &lt;- TRUE forward() }) pr$handle(&quot;GET&quot;, &quot;/&quot;, function(req){ paste(&quot;Am I filtered?&quot;, req$filtered) }) You can specify other options such as the serializer to use if the filter returns a value in the filter() method, as well. 8.4 Registering Hooks on a Router {router-hooks} Plumber routers support the notion of “hooks” that can be registered to execute some code at a particular point in the lifecycle of a request. Plumber routers currently support four hooks: preroute(data, req, res) postroute(data, req, res, value) preserialize(data, req, res, value) postserialize(data, req, res, value) In all of the above you have access to a disposable environment in the data parameter that is created as a temporary data store for each request. Hooks can store temporary data in these hooks that can be reused by other hooks processing this same request. One feature when defining hooks in Plumber routers is the ability to modify the returned value. The convention for such hooks is: any function that accepts a parameter named value is expected to return the new value. This could be an unmodified version of the value that was passed in, or it could be a mutated value. But in either case, if your hook accepts a parameter named value, whatever your hook returns will be used as the new value for the response. You can add hooks using the registerHook method, or you can add multiple hooks at once using the registerHooks method which takes a name list in which the names are the names of the hooks, and the values are the handlers themselves. pr &lt;- plumber$new() pr$registerHook(&quot;preroute&quot;, function(req){ cat(&quot;Routing a request for&quot;, req$PATH_INFO, &quot;...\\n&quot;) }) pr$registerHooks(list( preserialize=function(req, value){ print(&quot;About to serialize this value:&quot;) print(value) # Must return the value since we took one in. Here we&#39;re not choosing # to mutate it, but we could. value }, postserialize=function(res){ print(&quot;We serialized the value as:&quot;) print(res$body) } )) pr$handle(&quot;GET&quot;, &quot;/&quot;, function(){ 123 }) Making a GET request to / will print out various information from the three events for which we registered hooks. 8.5 Mounting &amp; Static File Routers Plumber routers can be “nested” by mounting one into another using the mount() method. This allows you to compartmentalize your API by paths which is a great technique for decomposing large APIs into smaller files. root &lt;- plumber$new() users &lt;- plumber$new(&quot;users.R&quot;) root$mount(&quot;/users&quot;, users) products &lt;- plumber$new(&quot;products.R&quot;) root$mount(&quot;/products&quot;, products) This is the same approach used for defining routers that serve a directory of static files. Static file routers are just a special case of Plumber routers created using PlumberStatic$new(). For example pr &lt;- plumber$new() stat &lt;- PlumberStatic$new(&quot;./myfiles&quot;) pr$mount(&quot;/assets&quot;, stat) This will make the files and directories stored in the ./myfiles directory available on your API under the /assets/ path. 8.6 Customizing a Router There are a handful of useful methods to be aware of to modify the behavior of a router. Using hooks to alter request processing has already been discussed, but additionally you can modify a router’s behavior using any of the following: setSerializer() - Sets the default serializer of the router. setErrorHandler() - Sets the error handler which gets invoked if any filter or endpoint generates an error. set404Handler() - Sets the handler that gets called if an incoming request can’t be served by any filter, endpoint, or sub-router. "],
["tips-tricks.html", "Chapter 9 Tips &amp; Tricks 9.1 Debugging 9.2 Testing 9.3 Organizing Large Applications 9.4 Swagger 9.5 Performance", " Chapter 9 Tips &amp; Tricks 9.1 Debugging If you’ve historically used R interactively, you may find it difficult to define functions that get executed at once without your input as Plumber requires. There are a couple of debugging techniques to be aware of when working on your Plumber APIs; these techniques are equally transferrable to debugging your R scripts, packages, or reports. 9.1.1 Print Debugging Most programmers first approach debugging by adding print statements to their code in order to inspect the state at some point. In R, print() or cat() can be used to print out some state. For instance, cat(&quot;i is currently: &quot;, i) could be inserted in your code to help you ensure that the variable i is what it should be at that point in your code. This approach is equally viable with Plumber. When developing your Plumber API in an interactive environment, this debugging output will be logged to the same terminal where you called run() on your API. In a non-interactive production environment, these messages will be included in the API server logs for later inspection. 9.1.2 Interactive Debugging Print debugging is an obvious starting point, but most developers eventually wish for something more powerful. In R, this capacity is built in to the debug() function. If you’re unfamiliar, debug() pauses the execution of some function and gives you an interactive session in which you can inspect the current value of internal variables or even proceed through your function one statement at a time. You can leverage debug() when developing your APIs locally by adding a debug() call in one of your filters or endpoints and then visiting your API in a client. This offers a powerful technique to use when you want to inspect multiple different variables or interact with the current state of things inside of your function. This is also a good way to get your hands dirty with Plumber and get better acquainted with how things behave at a low level. Consider the following API endpoint: #&#39; @get / function(req, res){ debug() list(a=123) } If you run this API locally and then visit the API in a web browser, you’ll see your R session switch into debug mode when the request arrives, allowing you to look at the objects contained inside your req and res objects. 9.2 Testing // TODO docs for testing 9.3 Organizing Large Applications // TODO docs for organizing large apps 9.4 Swagger // TODO Docs for Swagger 9.5 Performance //TODO Run some load tests and show the performance/overhead of running Plumber. "],
["all-annotations.html", "A All Annotations", " A All Annotations "],
["plumber-0-4-0-migration-guide.html", "B Plumber 0.4.0 Migration Guide", " B Plumber 0.4.0 Migration Guide Plumber underwent a series of breaking changes as a part of the 0.4.0 release. These changes were made as an attempt to rectify some earlier mistakes and as an attempt to take care of all foreseeable breaking changes for the Plumber package. There are a number of changes that users should consider when preparing to upgrade to plumber 0.4.0. Plumber no longer accepts external connections by default. The host parameter for the run() method now defaults to 127.0.0.1, meaning that Plumber will only listen for incoming requests from the local machine on which it’s running – not from any other machine on the network. This is done for security reasons so that you don’t accidentally expose a Plumber API that you’re developing to your entire network. To restore the old behavior in which Plumber listened for connections from any machine on the network, use $run(host=&quot;0.0.0.0&quot;). Note that if you’re deploying to an environment that includes an HTTP proxy (such as the DigitalOcean servers which use nginx), having Plumber listen only on 127.0.0.1 is likely the right default, as your proxy – not Plumber – is the one receiving external connections. Plumber no longer sets the Access-Control-Allow-Origin HTTP header to *. This was previously done for convenience but given the security implications we’re reversing this decision. The previous behavior would have allowed web browsers to make requests of your API from other domains using JavaScript if the request used only standard HTTP headers and were a GET, HEAD, or POST request. These requests will no longer work by default. If you wish to allow an endpoint to be accessible from other origins in a web browser, you can use res$setHeader(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;) in an endpoint or filter. Rather than setting the default port to 8000, the port is now randomly selected. This ensures that a shared server (like RStudio Server) will be able to support multiple people developing Plumber APIs concurrently without them having to manually identify an available port. This can be controlled by specifying the port parameter in the run() method or by setting the plumber.port option. The object-oriented model for Plumber routers has changed. If you’re calling any of the following methods on your Plumber router, you will need to modify your code to use the newer alternatives: addFilter, addEndpoint, addGlobalProcessor, and addAssets. The code around these functions has undergone a major rewrite and some breaking changes have been introduced. These four functions are still supported with a deprecation warning in 0.4.0, but support is only best-effort. Certain parameters on these methods are no longer supported, so you should thoroughly test any Plumber API that leverages any of these methods before deploying version 0.4.0. Updated documentation for using Plumber programmatically is now available. "],
["citations.html", "C Citations", " C Citations "]
]
